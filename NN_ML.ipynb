{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loan Default Prediction with Neural Networks\n",
        "\n",
        "This notebook implements a neural network model for predicting loan defaults using PyTorch. The model processes both numerical and categorical features using proper preprocessing techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-12-12T12:28:03.274787Z",
          "iopub.status.busy": "2024-12-12T12:28:03.274260Z",
          "iopub.status.idle": "2024-12-12T12:28:09.227204Z",
          "shell.execute_reply": "2024-12-12T12:28:09.226041Z",
          "shell.execute_reply.started": "2024-12-12T12:28:03.274716Z"
        },
        "id": "jqdyZktL3TQ8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Dataset Class\n",
        "We create a custom PyTorch Dataset class to handle our loan data efficiently. This class:\n",
        "- Converts features to PyTorch tensors\n",
        "- Handles both numpy arrays and sparse matrices\n",
        "- Supports both training data (with labels) and test data (without labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T12:29:08.415034Z",
          "iopub.status.busy": "2024-12-12T12:29:08.414046Z",
          "iopub.status.idle": "2024-12-12T12:29:08.423431Z",
          "shell.execute_reply": "2024-12-12T12:29:08.422132Z",
          "shell.execute_reply.started": "2024-12-12T12:29:08.414994Z"
        },
        "id": "PImNAMR_3TQ-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class LoanDataset(Dataset):\n",
        "    def __init__(self, features, labels=None):\n",
        "        # Convert features to tensor depending on input type\n",
        "        if isinstance(features, np.ndarray):\n",
        "            self.features = torch.FloatTensor(features)\n",
        "        else:  # Sparse matrix from ColumnTransformer\n",
        "            self.features = torch.FloatTensor(features.toarray())\n",
        "\n",
        "        # Convert labels to tensor if provided\n",
        "        self.labels = None if labels is None else torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels is None:\n",
        "            return self.features[idx]\n",
        "        return self.features[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Architecture\n",
        "Our model uses multiple fully connected layers with:\n",
        "- ReLU activation\n",
        "- Batch normalization for better training stability\n",
        "- Dropout for regularization\n",
        "- Binary classification output (Default vs No Default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T12:29:25.957888Z",
          "iopub.status.busy": "2024-12-12T12:29:25.956998Z",
          "iopub.status.idle": "2024-12-12T12:29:25.965867Z",
          "shell.execute_reply": "2024-12-12T12:29:25.964737Z",
          "shell.execute_reply.started": "2024-12-12T12:29:25.957847Z"
        },
        "id": "MYKpTqpn3TQ-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class LoanDefaultNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.3):\n",
        "        \"\"\"\n",
        "        Neural Network for loan default prediction\n",
        "        Args:\n",
        "            input_dim: Number of input features after preprocessing\n",
        "            hidden_dims: List of hidden layer dimensions\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        # Create hidden layers\n",
        "        for dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, dim),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(dim),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            prev_dim = dim\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_dim, 2))  # Binary classification\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "Here we:\n",
        "1. Load and preprocess the data\n",
        "2. Split features into numerical and categorical\n",
        "3. Apply appropriate scaling and encoding\n",
        "4. Create train/validation splits and data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T09:37:46.744323Z",
          "iopub.status.busy": "2024-12-09T09:37:46.743956Z",
          "iopub.status.idle": "2024-12-09T09:37:51.994016Z",
          "shell.execute_reply": "2024-12-09T09:37:51.992662Z",
          "shell.execute_reply.started": "2024-12-09T09:37:46.744289Z"
        },
        "id": "VI_9IpSe3TQ_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Define categorical and numerical features\n",
        "categorical_features = ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage',\n",
        "                       'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
        "numerical_features = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed',\n",
        "                     'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
        "\n",
        "# Create preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Prepare data\n",
        "X = preprocessor.fit_transform(data.drop(columns=['LoanID', 'Default']))\n",
        "X_test = preprocessor.transform(test_df.drop(columns=['LoanID']))\n",
        "y = data['Default'].values\n",
        "\n",
        "# Split training data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LoanDataset(X_train, y_train)\n",
        "val_dataset = LoanDataset(X_val, y_val)\n",
        "test_dataset = LoanDataset(X_test)  # No labels for test set\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Validation Functions\n",
        "These functions handle:\n",
        "- Training epochs\n",
        "- Validation\n",
        "- Prediction\n",
        "- Progress tracking with tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-19T19:00:37.693103Z",
          "iopub.status.busy": "2023-11-19T19:00:37.692844Z",
          "iopub.status.idle": "2023-11-19T19:00:37.703320Z",
          "shell.execute_reply": "2023-11-19T19:00:37.702576Z",
          "shell.execute_reply.started": "2023-11-19T19:00:37.693081Z"
        },
        "id": "vCzpZRlT3TQ_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc='Training'):\n",
        "        features, labels = batch\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc='Validating'):\n",
        "            features, labels = batch\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = (np.array(predictions) == np.array(actuals)).mean()\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "\n",
        "def predict(model, dataloader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features in tqdm(dataloader, desc='Predicting'):\n",
        "            features = features.to(device)\n",
        "            outputs = model(features)\n",
        "            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n",
        "Train the model with:\n",
        "- Cross-entropy loss\n",
        "- Adam optimizer\n",
        "- Model checkpointing for best validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctwXTRBz3TQ_",
        "outputId": "f1bd0200-7608-463e-e699-2f2149396462",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:02<00:00, 118.53it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 259.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 | Time: 3.03s\n",
            "Train Loss: 0.4533 | Val Loss: 0.3204 | Val Acc: 0.8843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 170.05it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 290.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/30 | Time: 2.18s\n",
            "Train Loss: 0.3307 | Val Loss: 0.3185 | Val Acc: 0.8841\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 170.62it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 282.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/30 | Time: 2.18s\n",
            "Train Loss: 0.3222 | Val Loss: 0.3167 | Val Acc: 0.8843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 174.13it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 276.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/30 | Time: 2.15s\n",
            "Train Loss: 0.3193 | Val Loss: 0.3168 | Val Acc: 0.8845\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:02<00:00, 141.89it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 178.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/30 | Time: 2.73s\n",
            "Train Loss: 0.3179 | Val Loss: 0.3162 | Val Acc: 0.8843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:02<00:00, 140.46it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 291.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/30 | Time: 2.57s\n",
            "Train Loss: 0.3164 | Val Loss: 0.3160 | Val Acc: 0.8846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 172.89it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 293.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/30 | Time: 2.14s\n",
            "Train Loss: 0.3160 | Val Loss: 0.3162 | Val Acc: 0.8850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 171.26it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 295.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/30 | Time: 2.16s\n",
            "Train Loss: 0.3153 | Val Loss: 0.3158 | Val Acc: 0.8849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 171.20it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 275.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/30 | Time: 2.18s\n",
            "Train Loss: 0.3150 | Val Loss: 0.3166 | Val Acc: 0.8848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 173.18it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 230.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/30 | Time: 2.22s\n",
            "Train Loss: 0.3148 | Val Loss: 0.3159 | Val Acc: 0.8847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:02<00:00, 118.24it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 261.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/30 | Time: 3.04s\n",
            "Train Loss: 0.3138 | Val Loss: 0.3160 | Val Acc: 0.8848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 175.14it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 284.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/30 | Time: 2.13s\n",
            "Train Loss: 0.3131 | Val Loss: 0.3165 | Val Acc: 0.8852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 172.23it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 287.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/30 | Time: 2.15s\n",
            "Train Loss: 0.3130 | Val Loss: 0.3161 | Val Acc: 0.8849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 173.74it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 293.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/30 | Time: 2.13s\n",
            "Train Loss: 0.3130 | Val Loss: 0.3164 | Val Acc: 0.8847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 170.25it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 287.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/30 | Time: 2.18s\n",
            "Train Loss: 0.3128 | Val Loss: 0.3176 | Val Acc: 0.8848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:02<00:00, 145.13it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 174.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/30 | Time: 2.69s\n",
            "Train Loss: 0.3119 | Val Loss: 0.3168 | Val Acc: 0.8842\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:02<00:00, 143.31it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 282.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/30 | Time: 2.54s\n",
            "Train Loss: 0.3123 | Val Loss: 0.3166 | Val Acc: 0.8847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 168.64it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 286.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/30 | Time: 2.20s\n",
            "Train Loss: 0.3115 | Val Loss: 0.3167 | Val Acc: 0.8850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 171.65it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 287.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/30 | Time: 2.16s\n",
            "Train Loss: 0.3114 | Val Loss: 0.3167 | Val Acc: 0.8853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 171.40it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 286.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/30 | Time: 2.16s\n",
            "Train Loss: 0.3111 | Val Loss: 0.3170 | Val Acc: 0.8850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 170.19it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 192.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/30 | Time: 2.32s\n",
            "Train Loss: 0.3103 | Val Loss: 0.3173 | Val Acc: 0.8851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:02<00:00, 119.72it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 285.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/30 | Time: 2.97s\n",
            "Train Loss: 0.3100 | Val Loss: 0.3177 | Val Acc: 0.8850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 170.24it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 251.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/30 | Time: 2.22s\n",
            "Train Loss: 0.3100 | Val Loss: 0.3175 | Val Acc: 0.8849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 172.67it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 265.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/30 | Time: 2.18s\n",
            "Train Loss: 0.3098 | Val Loss: 0.3191 | Val Acc: 0.8840\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 173.09it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 285.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/30 | Time: 2.15s\n",
            "Train Loss: 0.3093 | Val Loss: 0.3177 | Val Acc: 0.8849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 170.52it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 294.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/30 | Time: 2.17s\n",
            "Train Loss: 0.3094 | Val Loss: 0.3170 | Val Acc: 0.8851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:02<00:00, 132.83it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 162.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/30 | Time: 2.93s\n",
            "Train Loss: 0.3086 | Val Loss: 0.3175 | Val Acc: 0.8851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:01<00:00, 161.47it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 289.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/30 | Time: 2.28s\n",
            "Train Loss: 0.3085 | Val Loss: 0.3189 | Val Acc: 0.8846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:02<00:00, 146.38it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 271.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/30 | Time: 2.50s\n",
            "Train Loss: 0.3076 | Val Loss: 0.3181 | Val Acc: 0.8849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:02<00:00, 148.34it/s]\n",
            "Validating: 100%|██████████| 80/80 [00:00<00:00, 289.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/30 | Time: 2.45s\n",
            "Train Loss: 0.3074 | Val Loss: 0.3184 | Val Acc: 0.8850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "input_dim = X_train.shape[1]\n",
        "model = LoanDefaultNN(input_dim).to(device)\n",
        "\n",
        "# Training parameters\n",
        "n_epochs = 30\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "best_val_acc = 0\n",
        "for epoch in range(n_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs} | Time: {epoch_time:.2f}s\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Predictions\n",
        "Generate predictions on the test set using the best model and create submission file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0kdEBuS3TQ_",
        "outputId": "41e03e31-5d47-46bc-d1ab-732472709b2c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-2f1803d3d410>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n",
            "Predicting: 100%|██████████| 100/100 [00:00<00:00, 531.11it/s]\n"
          ]
        }
      ],
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "predictions = predict(model, test_loader)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'LoanID': test_df['LoanID'],\n",
        "    'Default': predictions\n",
        "})\n",
        "submission.to_csv('neural_network_submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6263400,
          "sourceId": 10146727,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30804,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
