{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Default Prediction Model\n",
    "\n",
    "This notebook implements a machine learning pipeline to predict loan defaults. The model uses various features like credit score, income, loan amount, and other borrower characteristics to predict whether a loan will default.\n",
    "\n",
    "## Data Processing Setup\n",
    "The following cell imports required libraries and sets up data preprocessing steps including:\n",
    "- Loading training and test datasets\n",
    "- One-hot encoding categorical features \n",
    "- Splitting data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Define the categorical features\n",
    "categorical_features = ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
    "\n",
    "# Create a ColumnTransformer to apply OneHotEncoding to categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep non-categorical columns as they are\n",
    ")\n",
    "\n",
    "# Fit and transform the training data, and transform the test data\n",
    "X = preprocessor.fit_transform(data.drop(columns=['LoanID', 'Default']))  \n",
    "X_tests = preprocessor.transform(test_df.drop(columns=['LoanID']))  \n",
    "y = data['Default']  # Target variable\n",
    "undropped_X_test = test_df  # Keep original test dataframe for submission\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def log_best_score(grid_search):\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"Best accuracy for model: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Hyperparameter Tuning\n",
    "\n",
    "We implement three different models with grid search for hyperparameter optimization:\n",
    "\n",
    "1. Decision Tree Classifier (currently commented out)\n",
    "2. XGBoost Classifier (currently commented out)\n",
    "3. Gradient Boosting Classifier (active)\n",
    "\n",
    "Each model's hyperparameter grid focuses on key parameters that affect model complexity and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best accuracy for model: 0.8862\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grids for each model\n",
    "dt_params = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "gb_params = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Train and tune Gradient Boosting model\n",
    "gb_grid_search = GridSearchCV(GradientBoostingClassifier(random_state=42), \n",
    "                             gb_params, \n",
    "                             scoring='accuracy', \n",
    "                             cv=5, \n",
    "                             n_jobs=-1, \n",
    "                             verbose=True)\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "log_best_score(gb_grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Generation\n",
    "\n",
    "The following cells define a utility function to generate submission files and preview the test data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(predictions, df):\n",
    "    \"\"\"Generate a submission file with loan IDs and predicted defaults\"\"\"\n",
    "    submission = pd.DataFrame({'LoanID': df['LoanID'], 'Default': predictions})\n",
    "    submission.to_csv(\"LendOrLose_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Prediction and Submission\n",
    "\n",
    "Use the best Gradient Boosting model to make predictions on the test set and generate submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_tuned = gb_grid_search.best_estimator_\n",
    "gb_prediction = gb_tuned.predict(X=X_tests)\n",
    "generate_submission(gb_prediction, undropped_X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
